import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics.cluster import normalized_mutual_info_score as NMI
import random

# Data Import
df = pd.read_csv('lung-cancer.csv', header=None)
output = open('Q1_output.txt', 'w')

# Replacing the unknown values by the mode value of that column
for index in range(1, df.shape[1]):
    df.iloc[df.iloc[:, index] == '?', index] = df.iloc[:, index].mode()[0]
    df.iloc[:, index] = pd.to_numeric(df.iloc[:, index])    # For converting any stray string data to numeric

X = df.iloc[:, 1:]              # Excluding the first column (class label)
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)  # Scaling the data for normalisation
print("The dimensions of the dataframe before performing PCA preserving 95% variance : ", X_scaled.shape, file = output)

# PCA scaling for preserving 95% variance
pca = PCA(n_components = 0.95)
pca.fit(X_scaled)
X_pca = pca.transform(X_scaled)
print("The dimensions of the dataframe after performing PCA preserving 95% variance : ", X_pca.shape, file = output)
print("\nThe variance obtained for each feature selected by PCA in sorted order :\n", pca.explained_variance_ratio_ * 100, file = output)
print("\nFinal variance obtained after including all the features selected by PCA : ", sum(pca.explained_variance_ratio_ * 100), file = output)

# Plot for PCA
actual_val = df.iloc[:, 0]      # The first column (class label)
plot = plt.scatter(X_pca[:,0], X_pca[:,1], c=actual_val)    # Considering the first two columns with max variance for plot
plt.legend(handles=plot.legend_elements()[0], labels=list(actual_val.unique()))
plt.xlabel('Column 0')
plt.ylabel('Column 1')
plt.savefig('Q1_plot_1.png')

def clustering(X_pca, centroids, k):
    '''
    Function for recalculating the clusters
    Input:
        X_pca : the dataframe obtained after applying PCA
        centroids : the dictionary of previously generated centroids
        k : the value of k used for KMeans
    Output:
        clusters : the dictionary of list of datapoints corresponding to each cluster
        centroids : the dictionary of recalculated centroids after generating new clusters
    '''
    clusters = {}
    for i in range(k):
        clusters[i] = []
    for data in X_pca:
        dist = []
        for j in range(k):
            dist.append(np.linalg.norm(data - centroids[j]))    # Distance calculation
        clusters[dist.index(min(dist))].append(data)    # Adding datapoint to the closest cluster
    # Recalculates the centroid position
    for i in range(k):
        centroids[i] = np.average(clusters[i], axis=0)
    return clusters, centroids

def cmp(new_dict, prev_dict):
    # Function (boolean) for comparing the data of two dictionaries of lists
    if len(new_dict) != len(prev_dict):
        return False
    for i in range(len(new_dict)):
        if len(new_dict[i]) != len(prev_dict[i]):
            return False
        for j in range(len(new_dict[i])):
            if (new_dict[i][j] != prev_dict[i][j]).any():
                return False
    return True

def KMeans(X_pca, actual_val, k):
    '''
    Function for KMeans clustering
    Input:
        X_pca : the dataframe obtained after applying PCA
        actual_val : the results of the dataframe X_pca
        k : the value of k used for KMeans
    Output:
        pred_val : the list of prediction results generated by the function
    '''
    centroids = {}
    clusters = {}
    X_rand_centroid = random.sample(range(X_pca.shape[0]), k)    # Selecting k random datapoints to be the centroids
    for i in range(k):
        clusters[i] = []
        centroids[i] = X_pca[X_rand_centroid[i],:]
    for data in X_pca:
        dist = []
        for j in range(k):
            dist.append(np.linalg.norm(data - centroids[j]))    # Distance calculation
        clusters[dist.index(min(dist))].append(data)    # Adding datapoint to the closest cluster
    for i in range(1000):   # max 1000 iterations
        new_clusters, new_centroids = clustering(X_pca, centroids, k)
        if cmp(new_clusters, clusters) or cmp(new_centroids, centroids):    # No change in the centroids or clusters
            break
        else:
            clusters = new_clusters
            centroids = new_centroids
    pred_val = []
    counter = 0
    for cluster in clusters.values():
        cluster_val = []
        for i in range(len(cluster)):
            cluster_val.append(actual_val[counter])
            counter+=1
        cluster_rep = max(set(cluster_val), key=cluster_val.count)      # Using mode value of a cluster as its representative
        for i in range(len(cluster_val)):
            pred_val.append(cluster_rep)
    return pred_val

k_list = []
NMI_list = []   # List of NMI values corresponding to each k from 2 to 8
print("\nThe list of values of NMI obtained for each k :", file = output)
for k in range(2,9):
    pred_val = KMeans(X_pca, actual_val, k)
    NMI_val = NMI(list(actual_val),list(pred_val))                      # NMI calculation
    k_list.append(k)
    NMI_list.append(NMI_val)
    print(k, " : ", NMI_val, file = output)
print("\nThe value of K for which maximum value of NMI is obtained is : ", k_list[NMI_list.index(max(NMI_list))], file = output)

# Plot for k vs normalised mutual information (NMI)
plt.plot(k_list, NMI_list)
plt.xlabel('K used for KMeans')
plt.ylabel('Calculated NMI values')
plt.title('NMI vs K')
plt.savefig('Q1_plot_2.png')

output.close()